# ---- Environment Setup ----

# The object can be created as a cuboid (box_object) or from mesh file (mesh_object). Comment out the one you don't need. 
box_object:
  shape: [1,1,1]
  contact_file: "assets/box_l1.csv" # path to sampled surface points on cube of unit length
  negate_contact_normal: false
  disabled_normal_directions: [[0,0,-1],[0,0,1],[0,1,0],[0,-1,0]]

maximum_surface_contact_points: 400

environment: # maximum 20 blocks
  block_1:
    dimension: [8,8,1]
    location: [0,0,-0.4999]

characteristic_length: 1

robot_object_friction_coefficient: 0.8
environment_object_friction_coefficient: 0.4

task_dynamics: "quasistatic" # "quasistatic", "quasidynamic", "none", "force_closure"
object_weight: [0,0,-1,0,0,0]
object_inertia: 
  [ 1, 0, 0, 0, 0, 0, 
    0, 1, 0, 0, 0, 0, 
    0, 0, 1, 0, 0, 0, 
    0, 0, 0, 0.167, 0, 0, 
    0, 0, 0, 0, 0.167, 0, 
    0, 0, 0, 0, 0, 0.167 ]

# ---- Robot ----

free_sphere_robot: 
  number_of_contacts: 1
  radius: 0.1
  patch_contact: true # false for point contacts

 

# ---- Start and Goal Pose ----
# pose [x, y, z, qx, qw, qz, qw]
object_start_pose: [0,0,0.5,0,0,0,1]
object_goal_pose: [1,0,0.5,0,0,0,1]

# ---- Run options ----
random_seed: -1

# ---- Planner options ----

refine_object_trajectory: # whether to refine the trajectory for 2nd level search
  enabled: false
  distance: 0.05

transition_pruning: false

rrt_options:
  max_samples: 50
  goal_biased_prob: 0.9
  goal_thr: 0.1
  extend_epsilon:
    translation: 0.7
    rotation_degree: 120
  weight:
    rotation: 1
    translation: 1
  sample:
    position_upper_bound: [2,2,0.5]
    position_lower_bound: [-2,-2,0.5]
    rotation: 
      SO3_enabled: true
      axis: [0,0,1] # if SO3_enabled is false, sample rotation on this axis

mcts_options:
  max_time: 5
  l1_1st_max_iterations: 10
  l1_max_iterations: 1
  l2_1st_max_iterations: 10
  l2_max_iterations: 1
  final_l2_1st_max_iterations: 20
  final_l2_max_iterations: 5

# ---- Reward and Probability ----
grasp_measure_charac_length: -1.0 # <0, disable grasp measure

level_1_reward: "CMGLevel1Reward"
level_2_reward: "CMGLevel2Reward"

# "inhand" (must have workspace limit) or "env"
action_probability_L2: "env"

# ---- Output and Visualization ----

# visualization options: 
# "csv": show the trajecotry in the result csv file
# "setup": show the task setup
# "save": plan a trajectory, saved in csv, no visualization
# "show": plan a trajectory and show, no save
# "save_n_show": plan a trajectory, saved in csv, and show the results

visualize_option: "save_n_show" 